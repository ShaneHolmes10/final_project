{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a09ebe3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3996/3996 [00:07<00:00, 507.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines loaded: 9560546\n",
      "                       movie             type         content\n",
      "0  The Mask_0110475_anno.txt           dialog                \n",
      "1  The Mask_0110475_anno.txt           dialog                \n",
      "2  The Mask_0110475_anno.txt           dialog                \n",
      "3  The Mask_0110475_anno.txt  speaker_heading        THE MASK\n",
      "4  The Mask_0110475_anno.txt           dialog                \n",
      "5  The Mask_0110475_anno.txt           dialog                \n",
      "6  The Mask_0110475_anno.txt           dialog                \n",
      "7  The Mask_0110475_anno.txt           dialog      Written by\n",
      "8  The Mask_0110475_anno.txt           dialog                \n",
      "9  The Mask_0110475_anno.txt  speaker_heading  Mark Verheiden\n",
      "type\n",
      "dialog             4609108\n",
      "text               2666910\n",
      "speaker_heading    1867723\n",
      "scene_heading       416805\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to BERT annotations\n",
    "bert_path = 'BERT_annotations/BERT_annotations/'\n",
    "\n",
    "# List to store all parsed lines\n",
    "all_data = []\n",
    "\n",
    "# Loop through all files\n",
    "for filename in tqdm(os.listdir(bert_path)):\n",
    "    filepath = os.path.join(bert_path, filename)\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Parse each line based on structure\n",
    "            # Example: \"dialog: Stay away from me!\"\n",
    "            if ':' in line:\n",
    "                parts = line.split(':', 1)\n",
    "                label = parts[0].strip()\n",
    "                text = parts[1].strip()\n",
    "                \n",
    "                all_data.append({\n",
    "                    'movie': filename,\n",
    "                    'type': label,  # dialog, text, speaker_heading, scene_heading\n",
    "                    'content': text\n",
    "                })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_data)\n",
    "print(f\"Total lines loaded: {len(df)}\")\n",
    "print(df.head(10))\n",
    "print(df['type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0fb1a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (4.57.1)\n",
      "Requirement already satisfied: torch in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (2.9.0)\n",
      "Requirement already satisfied: filelock in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from transformers) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from transformers) (2025.10.23)\n",
      "Requirement already satisfied: requests in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages (from requests->transformers) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "# Install transformers\n",
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a56ef8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cuda:0\n",
      "/home/shane/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'anger', 'score': 0.6600427031517029}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a pretrained emotion classifier\n",
    "emotion_classifier = pipeline(\n",
    "    \"text-classification\", \n",
    "    model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
    "    return_all_scores=False\n",
    ")\n",
    "\n",
    "# Emotions available:\n",
    "# anger, disgust, fear, joy, sadness, surprise, neutral\n",
    "\n",
    "# Test it on a single line\n",
    "text = \"Stay away from me!\"\n",
    "result = emotion_classifier(text)\n",
    "print(result)  # Returns emotion label and confidence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f109d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 4792210\n",
      "                                   movie  scene_id  \\\n",
      "0   10 Cloverfield Lane_1179933_anno.txt    389666   \n",
      "1   10 Cloverfield Lane_1179933_anno.txt    389667   \n",
      "2   10 Cloverfield Lane_1179933_anno.txt    389668   \n",
      "3   10 Cloverfield Lane_1179933_anno.txt    389670   \n",
      "4   10 Cloverfield Lane_1179933_anno.txt    389671   \n",
      "5   10 Cloverfield Lane_1179933_anno.txt    389671   \n",
      "6   10 Cloverfield Lane_1179933_anno.txt    389671   \n",
      "7   10 Cloverfield Lane_1179933_anno.txt    389671   \n",
      "8   10 Cloverfield Lane_1179933_anno.txt    389671   \n",
      "9   10 Cloverfield Lane_1179933_anno.txt    389671   \n",
      "10  10 Cloverfield Lane_1179933_anno.txt    389671   \n",
      "11  10 Cloverfield Lane_1179933_anno.txt    389671   \n",
      "12  10 Cloverfield Lane_1179933_anno.txt    389672   \n",
      "13  10 Cloverfield Lane_1179933_anno.txt    389672   \n",
      "14  10 Cloverfield Lane_1179933_anno.txt    389672   \n",
      "15  10 Cloverfield Lane_1179933_anno.txt    389672   \n",
      "16  10 Cloverfield Lane_1179933_anno.txt    389672   \n",
      "17  10 Cloverfield Lane_1179933_anno.txt    389672   \n",
      "18  10 Cloverfield Lane_1179933_anno.txt    389672   \n",
      "19  10 Cloverfield Lane_1179933_anno.txt    389672   \n",
      "\n",
      "                                        sentence_text  \n",
      "0   The Cellar by Josh Campbell & Matt Stuecken An...  \n",
      "1       BLURRED HEADLIGHTS flash across the screen --  \n",
      "2   A split second of unnerving silence and -- Lig...  \n",
      "3   A VEHICLE coming to rest -- A TIRE SPINNING up...  \n",
      "4                                         And SILENCE  \n",
      "5   The soft, rhythmic HUM of a distant generator ...  \n",
      "6                   Followed by a deep, pained BREATH  \n",
      "7   A dim light flickers, then steadies, revealing...  \n",
      "8   The light fades as consciousness slips away, r...  \n",
      "9   A slow FADE IN -- DIM FLUORESCENT LIGHT illumi...  \n",
      "10                     A thin blanket covers her body  \n",
      "11                             Her eyes remain closed  \n",
      "12  attractive and smart with a slight edge that d...  \n",
      "13  But right now she’s bandaged, bruised and bare...  \n",
      "14  Somewhere nearby -- the MUFFLED THUMP of a clo...  \n",
      "15      Michelle’s EYES SNAP OPEN -- glassy but aware  \n",
      "16                  She touches her temple and winces  \n",
      "17  Her fingers come away tinged with blood from t...  \n",
      "18                                    Her eyes adjust  \n",
      "19            Take in her surroundings -- Low ceiling  \n"
     ]
    }
   ],
   "source": [
    "from utils import preprocess_screenplay_data\n",
    "\n",
    "# Usage:\n",
    "df_consolidated = preprocess_screenplay_data(df)\n",
    "print(f\"Total sentences: {len(df_consolidated)}\")\n",
    "print(df_consolidated.head(20))\n",
    "df_consolidated.to_csv('sentences_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76e7d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "No progress bar\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\" from transformers import pipeline\n",
    "from time import time\n",
    "\n",
    "num_test_batches = 7992\n",
    "batch_size = 600\n",
    "test_size = num_test_batches * batch_size\n",
    "\n",
    "texts = df_consolidated['sentence_text'].tolist()[:test_size]  # Fixed: use sentence_text\n",
    "\n",
    "print(f\"Total sentences in corpus: {len(df_consolidated)}\")\n",
    "print(f\"Sentences to label: {len(texts)}\")\n",
    "\n",
    "start_time = time()\n",
    "results = emotion_classifier(\n",
    "    texts,\n",
    "    batch_size=batch_size,\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "end_time = time()\n",
    "\n",
    "print(f\"Completed in {(end_time - start_time)/60:.2f} minutes\")\n",
    "\n",
    "df_test = df_consolidated.iloc[:test_size].copy()\n",
    "df_test['emotion'] = [r['label'] for r in results]\n",
    "df_test['confidence'] = [r['score'] for r in results]\n",
    "\n",
    "df_test.to_csv('sentences_labeled_TEST.csv', index=False)\n",
    "print(df_test['emotion'].value_counts()) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65da94e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in corpus: 4792210\n",
      "Sentences to label: 4792210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling:   0%|          | 5400/4792210 [00:03<43:16, 1843.87it/s]  You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Labeling: 100%|██████████| 4792210/4792210 [52:55<00:00, 1508.94it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed in 52.93 minutes\n",
      "emotion\n",
      "neutral     2769474\n",
      "disgust      408864\n",
      "anger        396386\n",
      "surprise     362490\n",
      "fear         321574\n",
      "sadness      286987\n",
      "joy          246435\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "progress bar\n",
    "\"\"\"\n",
    "\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "\n",
    "num_test_batches = 7992\n",
    "batch_size = 600\n",
    "test_size = num_test_batches * batch_size\n",
    "\n",
    "texts = df_consolidated['sentence_text'].tolist()[:test_size]\n",
    "\n",
    "print(f\"Total sentences in corpus: {len(df_consolidated)}\")\n",
    "print(f\"Sentences to label: {len(texts)}\")\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "results = []\n",
    "pbar = tqdm(total=len(texts), desc=\"Labeling\")\n",
    "\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch = texts[i:i + batch_size]\n",
    "\n",
    "    out = emotion_classifier(\n",
    "        batch,\n",
    "        batch_size=batch_size,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    results.extend(out)\n",
    "    pbar.update(len(batch))\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "end_time = time()\n",
    "print(f\"Completed in {(end_time - start_time)/60:.2f} minutes\")\n",
    "\n",
    "df_test = df_consolidated.iloc[:test_size].copy()\n",
    "df_test['emotion'] = [r['label'] for r in results]\n",
    "df_test['confidence'] = [r['score'] for r in results]\n",
    "\n",
    "df_test.to_csv('sentences_labeled_TEST_SEFSEF.csv', index=False)\n",
    "print(df_test['emotion'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0793bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Look at the batch where it got stuck\n",
    "stuck_position = len(results)\n",
    "problem_batch = df_consolidated.iloc[stuck_position:stuck_position+600]\n",
    "\n",
    "print(f\"\\nBatch that caused hang (starting at {stuck_position}):\")\n",
    "print(problem_batch[['movie', 'sentence_text']].head(20))\n",
    "\n",
    "# Check for problematic sentences\n",
    "print(\"\\nSentence lengths in problem batch:\")\n",
    "print(problem_batch['sentence_text'].str.len().describe())\n",
    "\n",
    "# Look for weird characters\n",
    "print(\"\\nFirst few sentences:\")\n",
    "for idx, sent in problem_batch['sentence_text'].head(5).items():\n",
    "    print(f\"{idx}: {repr(sent)[:200]}\") \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
